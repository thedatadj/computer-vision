{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFIizfTnhIf4S8jHYGoH9w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thedatadj/computer-vision/blob/main/sentiment-analysis/deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loading\n",
        "Import:\n",
        "* Set of tweets with positive sentiment\n",
        "* Set of tweets with negative sentiment."
      ],
      "metadata": {
        "id": "p7pLVf9PFgcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import twitter_samples\n",
        "nltk.download('twitter_samples')\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idbtdkKkGbsA",
        "outputId": "29179e7e-df04-4227-e2c3-7c2330269aca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many tweets are there in each set?"
      ],
      "metadata": {
        "id": "ufjNiscDHIbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pn = len(all_positive_tweets)\n",
        "nn = len(all_negative_tweets)\n",
        "print(pn, nn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZ-2uhCDHK5-",
        "outputId": "17aaa7b2-0c47-43b0-8031-5b5836fc0b63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5000 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data split\n",
        "Create:\n",
        "* Training set `train_x`\n",
        "* Validation set `val_x`\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=19vSJsGft3277To0aw_YBpqLDAu2guDLa\" width=\"50%\" height=\"50%\">"
      ],
      "metadata": {
        "id": "4jaULN67Hb1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tool for this task\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "mpHL8wAEIBBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Divide all the positive tweets into two subsets:\n",
        "* Positive tweets for training (4,000)\n",
        "* Positive tweets for validation (1,000)"
      ],
      "metadata": {
        "id": "DJZ7DEapK16Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_pos = all_positive_tweets[:4000]\n",
        "val_pos = all_positive_tweets[4000:]"
      ],
      "metadata": {
        "id": "GPONiaC4Kl7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Divide all the negative tweets into two subsets:\n",
        "* Negative tweets for training (4,000)\n",
        "* Negative tweets for validation (1,000)"
      ],
      "metadata": {
        "id": "doH1wU88LY1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_neg = all_negative_tweets[:4000]\n",
        "val_neg = all_negative_tweets[4000:]"
      ],
      "metadata": {
        "id": "9fZJMv3kLrU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a features training dataset, consisting of:\n",
        "* Positive tweets for training (4,000)\n",
        "* Negative tweets for training (4,000)\n",
        "\n",
        "For a total of 8,000 training examples."
      ],
      "metadata": {
        "id": "C0JfD773LzHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = train_pos + train_neg"
      ],
      "metadata": {
        "id": "GuOS7xgnHNbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a dataset containing the labels/target of the training features.\n",
        "* 1 for the positive tweets\n",
        "* 0 for the negative tweets"
      ],
      "metadata": {
        "id": "bdozoB7yNALl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "positives = np.ones(4000)\n",
        "negatives = np.zeros(4000)\n",
        "train_y = np.append(positives, negatives)"
      ],
      "metadata": {
        "id": "ZoY8wG5BNNzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create validation set containing:\n",
        "* Last 1,000 positive tweets from all the positive tweets.\n",
        "* Last 1,000 negative tweets from all the negative tweets."
      ],
      "metadata": {
        "id": "-6eVq6zLNiuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_x = val_pos + val_neg"
      ],
      "metadata": {
        "id": "k7jgg1YbN0U7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the validation set of labels containing:\n",
        "* 1 for positive tweets\n",
        "* 0 for negative tweets"
      ],
      "metadata": {
        "id": "Owrch1_cN-Rb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_y = np.append(positives[:1000], negatives[:1000])"
      ],
      "metadata": {
        "id": "DDAPMnSAOGMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"length of train_x {len(train_x)}\")\n",
        "print(f\"length of val_x {len(val_x)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ht1pn2lNOQZ3",
        "outputId": "ce1fa8d6-4dbf-458a-e4ac-27a2a31254f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of train_x 8000\n",
            "length of val_x 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing\n",
        "Create a function that:\n",
        "* Removes unwanted characters from a tweet\n",
        "* Tokenize the tweet\n",
        "* Remove stopwords and punctuation\n",
        "* Stem the words"
      ],
      "metadata": {
        "id": "4W_7b5KYJCcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tool to remove unwanted characters from a string\n",
        "import re\n",
        "\n",
        "# Tool to tokenize the tweet\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tokenizer = TweetTokenizer(preserve_case=False,\n",
        "                           strip_handles=True,\n",
        "                           reduce_len=True)\n",
        "\n",
        "# Tool to remove stopwords and punctuation\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords_english = stopwords.words('english')\n",
        "import string\n",
        "\n",
        "# Tool to word stem\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tefvcP86OnxD",
        "outputId": "2e752d81-9f96-4011-c287-84994c00e1e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_tweet(tweet):\n",
        "    # Remove unwanted characters\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    # Tokenize tweet\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "    # Remove stopwords, punctuation and stem the words\n",
        "    tweet_clean = []\n",
        "    for token in tweet_tokens:\n",
        "        if (token not in stopwords_english and # remove stopwords\n",
        "            token not in string.punctuation): # remove punctuation\n",
        "            token_stem = stemmer.stem(token)\n",
        "            tweet_clean.append(token_stem)\n",
        "    return tweet_clean"
      ],
      "metadata": {
        "id": "z3LC1KaKI5N-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try out function that processes tweets\n",
        "print(\"Original tweet at training position 0\\n\")\n",
        "print(\"-->\", train_pos[0])\n",
        "\n",
        "print(\"\\n\\nTweet at training position 0 after processing:\\n\")\n",
        "process_tweet(train_pos[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvX3WUy6RqCo",
        "outputId": "54cc8cc8-c2c6-42c9-aeff-e87e4590e06e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original tweet at training position 0\n",
            "\n",
            "--> #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "\n",
            "\n",
            "Tweet at training position 0 after processing:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vocabulary\n",
        "Vocabulary of the training data.\n",
        "* Map each word to an index."
      ],
      "metadata": {
        "id": "ggWH_g6eSbp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vocab(train_x):\n",
        "    # Special tokens\n",
        "    vocab = {'__PAD__': 0,\n",
        "             '__</e>__': 1,\n",
        "             '__UNK__': 2}\n",
        "    for tweet in train_x:\n",
        "        tweet_processed = process_tweet(tweet)\n",
        "        for token in tweet_processed:\n",
        "            if token not in vocab:\n",
        "                index = len(vocab)\n",
        "                vocab[token] = index\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "E-epM8HlR7WC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = get_vocab(train_x)\n",
        "print(\"Total number of tokens in vocab:\", len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXDdbGLYZCYm",
        "outputId": "5b731dc3-4e4d-4b50-9536-30167b54cf8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of tokens in vocab: 9088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(vocab.items())[:4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKOwMrD6ZQ-2",
        "outputId": "a5196fa1-7eb9-4db3-8c46-ee978a272379"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('__PAD__', 0), ('__</e>__', 1), ('__UNK__', 2), ('followfriday', 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tweet to list of indexes\n",
        "Convert a tweet to a list containing its index representation.\n",
        "* Each token in the tweet is substituted by the index of such token in the vocabulary."
      ],
      "metadata": {
        "id": "tDVApbnUZoZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tweet_to_list(tweet, vocab_dict, unk_token='__UNK__'):\n",
        "    tokens = process_tweet(tweet)\n",
        "    lista = []\n",
        "    unk_ID = vocab_dict[unk_token]\n",
        "    for token in tokens:\n",
        "        token_id = vocab_dict.get(token, unk_ID)\n",
        "        lista.append(token_id)\n",
        "    return lista"
      ],
      "metadata": {
        "id": "EqvOLFViZWoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Actual tweet is\\n\", val_pos[1])\n",
        "print(\"\\nTensor of tweet:\\n\", tweet_to_list(val_pos[1], vocab_dict=vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOEyLtrdk7hY",
        "outputId": "72a95b9e-1259-46d1-ad1a-0845c711217d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual tweet is\n",
            " @heyclaireee is back! thnx God!!! i'm so happy :)\n",
            "\n",
            "Tensor of tweet:\n",
            " [443, 2, 303, 566, 56, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "Lo78Kf2AVvwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x1 = []\n",
        "for t in train_x:\n",
        "    tweet = tweet_to_list(t, vocab)\n",
        "    train_x1.append(tweet)"
      ],
      "metadata": {
        "id": "MJj03puZVyLb"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "lu5hUit-WK7y",
        "outputId": "95c7669e-1121-4500-aced-6871cfdcbc71"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x1[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okUMQ_tGWMfT",
        "outputId": "2db964e2-24ad-47ee-b2ce-b160edda06e0"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 4, 5, 6, 7, 8, 9]"
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Padding"
      ],
      "metadata": {
        "id": "1C7MJHAIWYuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max([len(t) for t in train_x1])\n",
        "max_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTPCVoUiWcwA",
        "outputId": "904735d1-e0ed-454b-a085-da1084bf881e"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_padded = []\n",
        "for tweet_idx in train_x1:\n",
        "    n_pad = max_len - len(tweet_idx)\n",
        "    pad_list = [0] * n_pad\n",
        "    tweet_padded = tweet_idx + pad_list\n",
        "    tweets_padded.append(tweet_padded)\n",
        "    inputs = np.array(tweets_padded)"
      ],
      "metadata": {
        "id": "jyGVdPOKWaHb"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x1[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzclUIhxX6-l",
        "outputId": "48a680d4-4395-49c8-e018-bf35a8a482ef"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 4, 5, 6, 7, 8, 9]"
            ]
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxZTRNBhXCIn",
        "outputId": "8f6484d3-6fb8-48d7-fcb0-fc99d9795828"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 4, 5, 6, 7, 8, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xKJtTO5X-UF",
        "outputId": "c9901e46-35fe-4abc-9ead-443a5042404a"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8000, 51)"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imN6d2S7YAez",
        "outputId": "143d50e5-06cd-4f13-e657-4a238c21f816"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8000,)"
            ]
          },
          "metadata": {},
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15HWbxuQdzrh",
        "outputId": "7bea76e7-2360-4914-e215-c4d5c5befea9"
      },
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(51,)"
            ]
          },
          "metadata": {},
          "execution_count": 233
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(len(vocab), 16, input_length=max_len),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(256),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy',\n",
        "              optimizer = 'adam',\n",
        "              metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "KWu439z4YFtO"
      },
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(inputs, train_y, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2Mj2yDaZWbx",
        "outputId": "32f13402-6db1-4391-d655-268e0819c9b1"
      },
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 0.8590 - accuracy: 0.7876\n",
            "Epoch 2/10\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 0.0709 - accuracy: 0.9906\n",
            "Epoch 3/10\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.0133 - accuracy: 0.9980\n",
            "Epoch 4/10\n",
            "250/250 [==============================] - 3s 11ms/step - loss: 0.0052 - accuracy: 0.9991\n",
            "Epoch 5/10\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 0.0029 - accuracy: 0.9992\n",
            "Epoch 6/10\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 0.0023 - accuracy: 0.9994\n",
            "Epoch 7/10\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 0.0019 - accuracy: 0.9995\n",
            "Epoch 8/10\n",
            "250/250 [==============================] - 2s 8ms/step - loss: 0.0017 - accuracy: 0.9995\n",
            "Epoch 9/10\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 0.0015 - accuracy: 0.9994\n",
            "Epoch 10/10\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.0020 - accuracy: 0.9996\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79ef01ce2620>"
            ]
          },
          "metadata": {},
          "execution_count": 235
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demostration"
      ],
      "metadata": {
        "id": "iKp_TQfZaG35"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a positive tweet that the model haven't seen."
      ],
      "metadata": {
        "id": "JFqqJf-2ec5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweet = val_x[1]\n",
        "tweet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ScTrrKrGZqUh",
        "outputId": "fba7bfc6-3e87-42c6-a895-0317e931f658"
      },
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"@heyclaireee is back! thnx God!!! i'm so happy :)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 253
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize it."
      ],
      "metadata": {
        "id": "OpyhLdc-eim3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweet0 = tweet_to_list(tweet, vocab)\n",
        "tweet0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6jN1tApZ9zo",
        "outputId": "26e8b63a-15e6-45ab-eedb-750f20d896e8"
      },
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[443, 2, 303, 566, 56, 9]"
            ]
          },
          "metadata": {},
          "execution_count": 247
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pad it."
      ],
      "metadata": {
        "id": "9ozmMxHreki-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_pad = max_len - len(tweet0)\n",
        "pad_list = [0] * n_pad\n",
        "tweet1 = tweet0 + pad_list\n",
        "tweet2 = np.array(tweet1)\n",
        "tweet2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIop6yqoaMwi",
        "outputId": "1e72eff5-159f-4e9e-b8c2-173acfdd1542"
      },
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([443,   2, 303, 566,  56,   9,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0])"
            ]
          },
          "metadata": {},
          "execution_count": 248
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make 2 dimensional"
      ],
      "metadata": {
        "id": "jP8Klyy0en1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweet3 = tweet2.reshape(1, 51)"
      ],
      "metadata": {
        "id": "cag4e4umaeKo"
      },
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet3.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kv0bmSXYcSXk",
        "outputId": "957e6f2c-110f-4399-a5a3-cd1efb1d064c"
      },
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 51)"
            ]
          },
          "metadata": {},
          "execution_count": 250
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = model.predict(tweet3)[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4FhKuL-ap-o",
        "outputId": "afcae7cc-76b2-44d7-9612-0972d341b8a3"
      },
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 25ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JQz_cfJcbe3",
        "outputId": "13e76f4a-2908-4ff8-8000-1166df9281e1"
      },
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.373132"
            ]
          },
          "metadata": {},
          "execution_count": 252
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model predicts is a positive tweet."
      ],
      "metadata": {
        "id": "sVeA-yfkeq7H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do the same with a negative tweet."
      ],
      "metadata": {
        "id": "aPWArN3geuge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ntweet = val_x[-1]\n",
        "ntweet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "v1cPUDRNet-o",
        "outputId": "8c0d70b8-9f53-4981-f926-ce51d35bc824"
      },
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'@eawoman As a Hull supporter I am expecting a misserable few weeks :-('"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 254
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ntweet0 = tweet_to_list(ntweet, vocab)\n",
        "ntweet0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psLV2CgVe1re",
        "outputId": "27267069-d089-46c2-fc14-dddd9a4dab34"
      },
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 219, 1375, 2, 8, 5747]"
            ]
          },
          "metadata": {},
          "execution_count": 255
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_pad = max_len - len(ntweet0)\n",
        "pad_list = [0] * n_pad\n",
        "ntweet1 = ntweet0 + pad_list\n",
        "ntweet2 = np.array(ntweet1)\n",
        "ntweet2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgHtPEiGe8fG",
        "outputId": "b2b7edec-b94d-4f3b-d2ac-4acbcbddaa7a"
      },
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   2,  219, 1375,    2,    8, 5747,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0])"
            ]
          },
          "metadata": {},
          "execution_count": 259
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ntweet3 = ntweet2.reshape(1, 51)\n",
        "prediction2 = model.predict(ntweet3)[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvTGgEVgfEEg",
        "outputId": "4e32d6e1-35e6-468a-d85b-1b1342ca341c"
      },
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjevJ5qyfNLE",
        "outputId": "1ea797ac-6546-466d-8edd-39f477b833cd"
      },
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.40451747"
            ]
          },
          "metadata": {},
          "execution_count": 261
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model predicted the tweet as negative."
      ],
      "metadata": {
        "id": "TMs8b9rffThR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CCOjFv5IfWKd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}